{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepONet for Acoustic Wave Propagation - Hands-On Tutorial\n",
    "\n",
    "**PhD Autumn School - Scientific Machine Learning**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this 1-hour hands-on exercise, you will:\n",
    "1. Understand the DeepONet architecture for operator learning\n",
    "2. Implement branch and trunk networks in PyTorch\n",
    "3. Implement the inner product operation that combines branch and trunk outputs\n",
    "4. Compare different activation functions (ReLU vs Sine)\n",
    "5. Investigate the impact of Fourier feature expansions\n",
    "\n",
    "## Introduction to DeepONet\n",
    "\n",
    "**Deep Operator Networks (DeepONet)** learn operators that map functions to functions, rather than just vectors to vectors like traditional neural networks.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "In acoustic wave propagation, we want to learn the operator $\\mathcal{G}$ that maps:\n",
    "- **Input function** $u(x)$: Initial pressure distribution (source configuration)\n",
    "- **Output function** $s(x, y, t)$: Pressure field at any location $(x, y)$ and time $t$\n",
    "\n",
    "Mathematically: $s = \\mathcal{G}(u)$\n",
    "\n",
    "### DeepONet Architecture\n",
    "\n",
    "```\n",
    "                    ┌─────────────┐\n",
    "u(x) ─────────────> │ Branch Net  │ ───> [b₁, b₂, ..., bₚ]\n",
    "  (function)        └─────────────┘            │\n",
    "                                               │\n",
    "                                               ├──> Inner Product ──> s(y)\n",
    "                                               │\n",
    "y = (x,y,t) ────> ┌─────────────┐            │\n",
    "  (coordinates)   │  Trunk Net  │ ───> [t₁, t₂, ..., tₚ]\n",
    "                  └─────────────┘\n",
    "```\n",
    "\n",
    "- **Branch network**: Encodes the input function $u$ into a latent representation\n",
    "- **Trunk network**: Encodes the query coordinates $(x, y, t)$\n",
    "- **Inner product**: Combines both representations: $s(y) = \\sum_{i=1}^{p} b_i(u) \\cdot t_i(y) + b_0$\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "By learning this decomposition, the network can:\n",
    "- Generalize to new source configurations $u$ (never seen during training)\n",
    "- Evaluate at arbitrary query locations $(x, y, t)$\n",
    "- Reduce computational cost compared to traditional PDE solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import project data handlers\n",
    "from deeponet_acoustics.datahandlers.datagenerators import (\n",
    "    DataH5Compact,\n",
    "    DatasetStreamer,\n",
    "    pytorch_collate  # PyTorch collator for data loading\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading (Pre-filled)\n",
    "\n",
    "We'll use the existing data loading infrastructure to load 2D acoustic wave propagation data.\n",
    "The data consists of:\n",
    "- Training simulations with different source configurations\n",
    "- Pressure fields computed at various spatial locations and time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Update these paths to point to your data\n",
    "train_data_path = \"path/to/training/data\"  # TODO: Update this path\n",
    "test_data_path = \"path/to/test/data\"       # TODO: Update this path\n",
    "\n",
    "# Load training data\n",
    "data_train = DataH5Compact(\n",
    "    train_data_path,\n",
    "    tmax=0.015,      # Maximum time to consider\n",
    "    t_norm=343.0,    # Speed of sound for normalization\n",
    "    flatten_ic=True, # Flatten initial conditions\n",
    "    norm_data=True,  # Normalize spatial coordinates\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "data_test = DataH5Compact(\n",
    "    test_data_path,\n",
    "    tmax=0.015,\n",
    "    t_norm=343.0,\n",
    "    flatten_ic=True,\n",
    "    norm_data=True,\n",
    ")\n",
    "\n",
    "print(f\"Training sources: {data_train.N}\")\n",
    "print(f\"Test sources: {data_test.N}\")\n",
    "print(f\"Mesh points: {data_train.P_mesh}\")\n",
    "print(f\"Time steps: {len(data_train.tsteps)}\")\n",
    "print(f\"Input (u) shape: {data_train.u_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "batch_size_branch = 4    # Number of different sources per batch\n",
    "batch_size_coord = 100   # Number of coordinate points per batch\n",
    "\n",
    "dataset_train = DatasetStreamer(data_train, batch_size_coord=batch_size_coord)\n",
    "dataset_test = DatasetStreamer(data_test, batch_size_coord=-1)  # Full dataset for testing\n",
    "\n",
    "# Use pytorch_collate to convert to PyTorch tensors\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size_branch,\n",
    "    shuffle=True,\n",
    "    collate_fn=pytorch_collate,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=pytorch_collate,\n",
    ")\n",
    "\n",
    "# Get a sample batch to understand dimensions\n",
    "sample_batch = next(iter(dataloader_train))\n",
    "(u_sample, y_sample), s_sample, _, _ = sample_batch\n",
    "\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  u (branch input): {u_sample.shape}  # [batch_branch, u_dim]\")\n",
    "print(f\"  y (trunk input):  {y_sample.shape}  # [batch_branch, batch_coord, coord_dim]\")\n",
    "print(f\"  s (output):       {s_sample.shape}  # [batch_branch, batch_coord]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fourier Feature Expansion (Pre-filled)\n",
    "\n",
    "Fourier features help neural networks learn high-frequency functions by mapping inputs through:\n",
    "$$\\text{FourierFeatures}(y) = [y, \\cos(2\\pi f_1 y), \\sin(2\\pi f_1 y), \\cos(2\\pi f_2 y), \\sin(2\\pi f_2 y), ...]$$\n",
    "\n",
    "This is particularly useful for learning periodic or oscillatory patterns like acoustic waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_feature_expansion(freqs=[]):\n",
    "    \"\"\"\n",
    "    Create a Fourier feature expansion function.\n",
    "    \n",
    "    Args:\n",
    "        freqs: List of frequencies for Fourier features\n",
    "               Empty list means no Fourier features (just return input)\n",
    "    \n",
    "    Returns:\n",
    "        Function that applies Fourier feature expansion\n",
    "    \"\"\"\n",
    "    if len(freqs) == 0:\n",
    "        return lambda y: y\n",
    "    \n",
    "    def expand(y):\n",
    "        # y shape: [batch, coord_dim] or [batch, n_points, coord_dim]\n",
    "        features = [y]\n",
    "        for f in freqs:\n",
    "            features.append(torch.cos(2 * np.pi * f * y))\n",
    "            features.append(torch.sin(2 * np.pi * f * y))\n",
    "        return torch.cat(features, dim=-1)\n",
    "    \n",
    "    return expand\n",
    "\n",
    "# Example: no Fourier features\n",
    "feat_fn_none = fourier_feature_expansion(freqs=[])\n",
    "\n",
    "# Example: with Fourier features at specific frequencies\n",
    "feat_fn_fourier = fourier_feature_expansion(freqs=[1.0, 2.0])\n",
    "\n",
    "# Test\n",
    "test_input = torch.randn(4, 3)  # [batch=4, coord_dim=3]\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Without Fourier features: {feat_fn_none(test_input).shape}\")\n",
    "print(f\"With Fourier features: {feat_fn_fourier(test_input).shape}  # 3 + 2*2*3 = 15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Network Components - YOUR TASK\n",
    "\n",
    "Now it's your turn! Implement the branch and trunk networks.\n",
    "\n",
    "### Task 3.1: Branch Network\n",
    "\n",
    "The branch network takes the initial condition $u$ (a flattened vector) and produces a latent representation.\n",
    "\n",
    "**TODO**: Complete the `BranchNet` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Branch network for DeepONet.\n",
    "    \n",
    "    Architecture: input -> hidden -> hidden -> ... -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers, activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimension of input function u\n",
    "            hidden_dim: Number of neurons in hidden layers\n",
    "            output_dim: Dimension of latent representation (p)\n",
    "            num_hidden_layers: Number of hidden layers\n",
    "            activation: 'relu' or 'sine'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Create a list of layers\n",
    "        # Structure: input_dim -> hidden_dim -> hidden_dim -> ... -> output_dim\n",
    "        # Hint: Use nn.Linear for each layer\n",
    "        # Hint: You'll need num_hidden_layers + 1 total layers\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # === YOUR CODE HERE ===\n",
    "        # First layer: input_dim -> hidden_dim\n",
    "        \n",
    "        \n",
    "        # Hidden layers: hidden_dim -> hidden_dim (repeat num_hidden_layers-1 times)\n",
    "        \n",
    "        \n",
    "        # Output layer: hidden_dim -> output_dim\n",
    "        \n",
    "        # === END YOUR CODE ===\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "        # Select activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'sine':\n",
    "            self.activation = lambda x: torch.sin(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, u):\n",
    "        \"\"\"\n",
    "        Forward pass through branch network.\n",
    "        \n",
    "        Args:\n",
    "            u: Input function [batch_size, input_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Branch latent representation [batch_size, output_dim]\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # Apply activation after each layer EXCEPT the last one\n",
    "        \n",
    "        x = u\n",
    "        \n",
    "        # === YOUR CODE HERE ===\n",
    "        \n",
    "        \n",
    "        # === END YOUR CODE ===\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Trunk Network\n",
    "\n",
    "The trunk network takes coordinate points $(x, y, t)$ and produces a latent representation.\n",
    "\n",
    "**TODO**: Complete the `TrunkNet` class below. It's very similar to BranchNet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrunkNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Trunk network for DeepONet.\n",
    "    \n",
    "    Architecture: input -> hidden -> hidden -> ... -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers, activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimension of coordinate input y (e.g., 3 for x,y,t)\n",
    "            hidden_dim: Number of neurons in hidden layers\n",
    "            output_dim: Dimension of latent representation (p) - must match branch output!\n",
    "            num_hidden_layers: Number of hidden layers\n",
    "            activation: 'relu' or 'sine'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Implement trunk network layers (same structure as branch network)\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # === YOUR CODE HERE ===\n",
    "        \n",
    "        \n",
    "        # === END YOUR CODE ===\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'sine':\n",
    "            self.activation = lambda x: torch.sin(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        Forward pass through trunk network.\n",
    "        \n",
    "        Args:\n",
    "            y: Coordinate points [batch_size, n_points, input_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Trunk latent representation [batch_size, n_points, output_dim]\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # Note: y has an extra dimension (n_points) compared to branch input\n",
    "        \n",
    "        x = y\n",
    "        \n",
    "        # === YOUR CODE HERE ===\n",
    "        \n",
    "        \n",
    "        # === END YOUR CODE ===\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DeepONet Model - YOUR TASK\n",
    "\n",
    "### Task 4.1: Inner Product Operation\n",
    "\n",
    "The key innovation of DeepONet is combining branch and trunk outputs via an inner product:\n",
    "\n",
    "$$s(y) = \\sum_{i=1}^{p} b_i(u) \\cdot t_i(y) + b_0$$\n",
    "\n",
    "where:\n",
    "- $b_i(u)$ are the branch network outputs (one value per source $u$)\n",
    "- $t_i(y)$ are the trunk network outputs (one value per coordinate $y$)\n",
    "- $b_0$ is a learnable bias term\n",
    "\n",
    "**TODO**: Complete the inner product operation in the `DeepONet` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepONet(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Operator Network combining branch and trunk networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, branch_net, trunk_net, use_bias=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            branch_net: Branch network (BranchNet instance)\n",
    "            trunk_net: Trunk network (TrunkNet instance)\n",
    "            use_bias: Whether to use bias term b0\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.branch_net = branch_net\n",
    "        self.trunk_net = trunk_net\n",
    "        \n",
    "        # Learnable bias term\n",
    "        self.b0 = nn.Parameter(torch.zeros(1)) if use_bias else 0.0\n",
    "    \n",
    "    def forward(self, u, y):\n",
    "        \"\"\"\n",
    "        Forward pass through DeepONet.\n",
    "        \n",
    "        Args:\n",
    "            u: Branch input [batch_branch, u_dim]\n",
    "            y: Trunk input [batch_branch, n_points, coord_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Predictions [batch_branch, n_points]\n",
    "        \"\"\"\n",
    "        # Get branch and trunk latent representations\n",
    "        branch_output = self.branch_net(u)  # [batch_branch, p]\n",
    "        trunk_output = self.trunk_net(y)    # [batch_branch, n_points, p]\n",
    "        \n",
    "        # TODO: Implement inner product operation\n",
    "        # Goal: Compute sum_i( branch_output[:,i] * trunk_output[:,:,i] )\n",
    "        # Result should have shape [batch_branch, n_points]\n",
    "        # \n",
    "        # Hints:\n",
    "        #   - branch_output has shape [batch_branch, p]\n",
    "        #   - trunk_output has shape [batch_branch, n_points, p]\n",
    "        #   - You need to align dimensions for element-wise multiplication\n",
    "        #   - Use .unsqueeze() to add dimensions\n",
    "        #   - Use .sum(dim=...) to sum over the p dimension\n",
    "        \n",
    "        # === YOUR CODE HERE ===\n",
    "        \n",
    "        s_pred = None  # Replace with your implementation\n",
    "        \n",
    "        # === END YOUR CODE ===\n",
    "        \n",
    "        # Add bias\n",
    "        if isinstance(self.b0, nn.Parameter):\n",
    "            s_pred = s_pred + self.b0\n",
    "        \n",
    "        return s_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Let's test your implementation with random inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test networks\n",
    "test_branch = BranchNet(input_dim=100, hidden_dim=64, output_dim=40, num_hidden_layers=2, activation='relu')\n",
    "test_trunk = TrunkNet(input_dim=3, hidden_dim=64, output_dim=40, num_hidden_layers=2, activation='relu')\n",
    "test_model = DeepONet(test_branch, test_trunk)\n",
    "\n",
    "# Create random test data\n",
    "test_u = torch.randn(4, 100)   # [batch_branch=4, u_dim=100]\n",
    "test_y = torch.randn(4, 50, 3) # [batch_branch=4, n_points=50, coord_dim=3]\n",
    "\n",
    "# Forward pass\n",
    "test_output = test_model(test_u, test_y)\n",
    "\n",
    "print(f\"Branch output shape: {test_branch(test_u).shape}\")\n",
    "print(f\"Trunk output shape: {test_trunk(test_y).shape}\")\n",
    "print(f\"DeepONet output shape: {test_output.shape}\")\n",
    "print(f\"Expected output shape: [4, 50]\")\n",
    "\n",
    "assert test_output.shape == (4, 50), \"Output shape is incorrect!\"\n",
    "print(\"\\n✓ Test passed! Your implementation is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop (Pre-filled)\n",
    "\n",
    "Now we'll train the model. The training loop is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deeponet(model, dataloader_train, dataloader_test, num_epochs, learning_rate, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train DeepONet model.\n",
    "    \n",
    "    Args:\n",
    "        model: DeepONet instance\n",
    "        dataloader_train: Training data loader\n",
    "        dataloader_test: Test data loader\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        device: 'cpu' or 'cuda'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'test_loss': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataloader_train:\n",
    "            (u, y), s_true, _, _ = batch\n",
    "            u, y, s_true = u.to(device), y.to(device), s_true.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            s_pred = model(u, y)\n",
    "            loss = criterion(s_pred, s_true)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_epoch += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        train_loss_epoch /= num_batches\n",
    "        history['train_loss'].append(train_loss_epoch)\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss_epoch = 0\n",
    "        num_test_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader_test:\n",
    "                (u, y), s_true, _, _ = batch\n",
    "                u, y, s_true = u.to(device), y.to(device), s_true.to(device)\n",
    "                \n",
    "                s_pred = model(u, y)\n",
    "                loss = criterion(s_pred, s_true)\n",
    "                \n",
    "                test_loss_epoch += loss.item()\n",
    "                num_test_batches += 1\n",
    "        \n",
    "        test_loss_epoch /= num_test_batches\n",
    "        history['test_loss'].append(test_loss_epoch)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss_epoch:.6f}, Test Loss: {test_loss_epoch:.6f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and test loss over epochs.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.semilogy(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    plt.semilogy(history['test_loss'], label='Test Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('MSE Loss', fontsize=12)\n",
    "    plt.title('Training History', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiments - YOUR TASK\n",
    "\n",
    "Now that you have a working DeepONet implementation, let's investigate how design choices affect performance!\n",
    "\n",
    "### Experiment 6.1: ReLU vs Sine Activation\n",
    "\n",
    "**Research Question**: Do sine activations help for learning smooth, periodic acoustic wave functions?\n",
    "\n",
    "**TODO**: Train two models and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim_branch = data_train.u_shape[0]  # From data\n",
    "input_dim_trunk = 3  # x, y, t coordinates\n",
    "hidden_dim = 64\n",
    "output_dim = 64  # latent dimension p\n",
    "num_hidden_layers = 3\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# === YOUR CODE HERE ===\n",
    "# Create two models:\n",
    "# 1. model_relu: using activation='relu'\n",
    "# 2. model_sine: using activation='sine'\n",
    "\n",
    "# Model 1: ReLU activation\n",
    "\n",
    "\n",
    "# Model 2: Sine activation\n",
    "\n",
    "\n",
    "# Train both models\n",
    "print(\"Training model with ReLU activation...\")\n",
    "# history_relu = ...\n",
    "\n",
    "print(\"\\nTraining model with Sine activation...\")\n",
    "# history_sine = ...\n",
    "\n",
    "# === END YOUR CODE ===\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogy(history_relu['train_loss'], label='ReLU - Train', linewidth=2)\n",
    "plt.semilogy(history_sine['train_loss'], label='Sine - Train', linewidth=2, linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(history_relu['test_loss'], label='ReLU - Test', linewidth=2)\n",
    "plt.semilogy(history_sine['test_loss'], label='Sine - Test', linewidth=2, linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Test Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(f\"\\nFinal Test Loss:\")\n",
    "print(f\"  ReLU: {history_relu['test_loss'][-1]:.6f}\")\n",
    "print(f\"  Sine: {history_sine['test_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6.2: Impact of Fourier Features\n",
    "\n",
    "**Research Question**: Do Fourier features improve the network's ability to learn high-frequency wave patterns?\n",
    "\n",
    "**TODO**: Train models with and without Fourier features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# Create datasets with different feature expansions:\n",
    "# 1. No Fourier features: feat_fn_none = fourier_feature_expansion(freqs=[])\n",
    "# 2. With Fourier features: feat_fn_fourier = fourier_feature_expansion(freqs=[1.0, 2.0, 4.0])\n",
    "\n",
    "# Create DatasetStreamer with y_feat_extract_fn parameter\n",
    "# Hint: dataset = DatasetStreamer(data_train, batch_size_coord=100, y_feat_extract_fn=feat_fn)\n",
    "\n",
    "# Create DataLoaders\n",
    "\n",
    "# Create models (remember to adjust input_dim_trunk based on Fourier features!)\n",
    "# Without Fourier: input_dim_trunk = 3\n",
    "# With Fourier [1.0, 2.0, 4.0]: input_dim_trunk = 3 + 2*3*3 = 21\n",
    "\n",
    "# Train and compare\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Questions\n",
    "\n",
    "After running the experiments, discuss:\n",
    "\n",
    "1. **Activation Functions**:\n",
    "   - Which activation (ReLU vs Sine) performed better? Why might this be?\n",
    "   - How did convergence speed differ between the two?\n",
    "   - What physical properties of acoustic waves might favor one activation over another?\n",
    "\n",
    "2. **Fourier Features**:\n",
    "   - Did Fourier features improve performance? By how much?\n",
    "   - What is the computational cost of Fourier features (hint: look at input dimensions)?\n",
    "   - When would you recommend using Fourier features?\n",
    "\n",
    "3. **DeepONet Architecture**:\n",
    "   - Why is the operator learning approach useful for PDEs?\n",
    "   - What are the advantages of DeepONet vs traditional PDE solvers?\n",
    "   - What are the limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bonus Challenges (If Time Permits)\n",
    "\n",
    "If you finish early, try these extensions:\n",
    "\n",
    "### Challenge 1: Visualization\n",
    "Visualize the predicted vs true pressure field for a test case. Create an animation over time.\n",
    "\n",
    "### Challenge 2: Architecture Search\n",
    "Experiment with:\n",
    "- Different latent dimensions (p)\n",
    "- Different numbers of hidden layers\n",
    "- Different hidden layer widths\n",
    "\n",
    "### Challenge 3: Advanced Features\n",
    "- Implement adaptive learning rate scheduling\n",
    "- Try different optimizers (SGD, AdamW)\n",
    "- Add L2 regularization\n",
    "\n",
    "### Challenge 4: Physical Constraints\n",
    "- How could you incorporate physical constraints (e.g., energy conservation) into the loss function?\n",
    "- Implement a physics-informed loss term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you:\n",
    "- ✓ Learned about DeepONet architecture for operator learning\n",
    "- ✓ Implemented branch and trunk networks in PyTorch\n",
    "- ✓ Implemented the crucial inner product operation\n",
    "- ✓ Compared ReLU vs Sine activations for smooth functions\n",
    "- ✓ Investigated the impact of Fourier feature expansions\n",
    "\n",
    "**Key Takeaways**:\n",
    "1. DeepONet learns operators (function-to-function mappings) rather than just vectors\n",
    "2. The architecture separates encoding of inputs (branch) and query locations (trunk)\n",
    "3. Activation functions and feature engineering significantly impact performance\n",
    "4. Operator learning is a powerful tool for surrogate modeling of PDEs\n",
    "\n",
    "**Further Reading**:\n",
    "- Original DeepONet paper: Lu et al. (2021), \"Learning nonlinear operators via DeepONet\"\n",
    "- Physics-Informed DeepONet: Wang et al. (2021)\n",
    "- Fourier Features: Tancik et al. (2020), \"Fourier Features Let Networks Learn High Frequency Functions\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
